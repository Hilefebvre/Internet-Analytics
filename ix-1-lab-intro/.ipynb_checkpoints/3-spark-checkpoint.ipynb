{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "[Spark](https://spark.apache.org/docs/1.6.2/index.html) (we are going to use version 1.6.2) is a fast and general-purpose cluster computing system, that allows to process large datasets using several machines. In the era of the *big data*, it has become a necessity to be able to distribute computing power and process data in parallel. We introduce here the basics of the framework.\n",
    "\n",
    "In any case, you should have a look at the [official tutorial](https://spark.apache.org/docs/1.6.2/programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop File System (HDFS)\n",
    "\n",
    "The main abstraction Spark offers is the *Resisilient Distributed Dataset* (RDD), which is a collection of elements distributed across the nodes of the cluster. The idea is that you typically **do not** load them in the memory, but rather process them *lazily*. They are initialized with files in the *Hadoop File System* (HDFS). \n",
    "\n",
    "Let us first see what there is in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - hlefebvr hadoop          0 2018-02-21 11:36 .sparkStaging\r\n",
      "-rw-r--r--   3 hlefebvr hadoop    2147813 2018-02-21 10:37 election-day-tweets.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* a line starting with `!` indicates a shell command. You can hence run this same command in a bash terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add a dataset to HFDS. In the `data` folder, there is text file containing 30000 tweets collected from the [Inauguration Day](https://en.wikipedia.org/wiki/United_States_presidential_inauguration) on January 20, 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `election-day-tweets.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put data/election-day-tweets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that the file is indeed in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - hlefebvr hadoop          0 2018-02-21 11:36 .sparkStaging\r\n",
      "-rw-r--r--   3 hlefebvr hadoop    2147813 2018-02-21 10:37 election-day-tweets.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* you can remove files from HDFS using `hdfs dfs -rm FILE`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset processing\n",
    "\n",
    "We can now start playing with the tweets. We will instantiate the dataset from the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = sc.textFile('election-day-tweets.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tweets` variable is now an RDD. An RDD is *resilient* to faults by storing the sequence of operations applied to it so that it can easily recreate the state of the data structure in case of a problem. It can also be logically cut to be *distributed* on several machines.\n",
    "\n",
    "The `sc` object is a special object, namely a [Spark Context](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext) obeject, that we configured to be available everywhere in your Python environment. It is the main interface with Spark that has numerous useful methods. Check the documentation!\n",
    "\n",
    "Let us display a few tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: @Lawrence @HillaryClinton Two first  @SenSchumer tomorrow. @TheLastWord #brooklyn  TheRealAmerica #Vote #Democrats #NastyWomenVote #Senate\n",
      "1: My @latimesopinion op-ed on historic #California #Senate race. First time an elected woman senator succeeds another.\n",
      "2: https://t.co/cbjQTK0Q1V\n",
      "3: #Senate Wisconsin Senate Preview: Johnson vs. Feingold\n",
      "4: If Rubio Wins and #Trump Loses in #Florida... #HillaryClinton #Senate #RepublicanPrimary #Senaterace #Miami... https://t.co/zIeNEcVnMO\n",
      "5: #Senate Wisconsin Senate Preview: Johnson vs. Feingold\n",
      "6: bob day is an honest  person   #senate patterson . a loss to the senate\n",
      "7: Make Republicans #PayAPrice!\n",
      "8:  ðŸ’™ðŸ‡ºðŸ‡¸#VoteBLUEðŸ”ƒtheBallotðŸ‡ºðŸ‡¸ðŸ’™\n",
      "9:  #Congress #Senate #FlipItDem\n"
     ]
    }
   ],
   "source": [
    "some_tweets = tweets.take(10)\n",
    "for i, tweet in enumerate(some_tweets):\n",
    "    print(\"%d: %s\" % (i, tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the `tweets` dataset can be acted on by dataset **operations**. RDDs support two types of operations: *transformations*, which create a new RDD from an existing one, and *actions*, which return a value to the driver program after running a computation on the RDD. For instance, we can `count` the number of tweets. Is it a transofrmation or an action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also `filter` the lines matching a specific string. Is it a transformation or an action ?\n",
    "\n",
    "*Note*: `lower()` is a `str` method that converts the string to lower case. What happens if you do not do that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_trump = tweets.filter(lambda line: \"trump\" in line.lower())\n",
    "lines_with_trump.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical processing pattern is [MapReduce](https://en.wikipedia.org/wiki/MapReduce), as popularized by Hadoop. The idea is that you apply a function to your dataset that filters or sorts it (**map**), and then combine the ouputs by computing something (**reduce**). The typical example is a word count. So, how many times each word occur among our tweets?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Lawrence',\n",
       " '@HillaryClinton',\n",
       " 'Two',\n",
       " 'first',\n",
       " '@SenSchumer',\n",
       " 'tomorrow.',\n",
       " '@TheLastWord',\n",
       " '#brooklyn',\n",
       " 'TheRealAmerica',\n",
       " '#Vote',\n",
       " '#Democrats',\n",
       " '#NastyWomenVote',\n",
       " '#Senate',\n",
       " 'My',\n",
       " '@latimesopinion',\n",
       " 'op-ed',\n",
       " 'on',\n",
       " 'historic',\n",
       " '#California',\n",
       " '#Senate']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split every line into words and flattens the result (aggregate them all together)\n",
    "words = tweets.flatMap(lambda line: line.split())\n",
    "words.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@Lawrence', 1),\n",
       " ('@HillaryClinton', 1),\n",
       " ('Two', 1),\n",
       " ('first', 1),\n",
       " ('@SenSchumer', 1),\n",
       " ('tomorrow.', 1),\n",
       " ('@TheLastWord', 1),\n",
       " ('#brooklyn', 1),\n",
       " ('TheRealAmerica', 1),\n",
       " ('#Vote', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign 1 to each word\n",
    "counts = words.map(lambda word: (word, 1))\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@HillaryClinton', 121),\n",
       " ('Two', 8),\n",
       " ('#Democrats', 20),\n",
       " ('op-ed', 1),\n",
       " ('historic', 10),\n",
       " ('race.', 5),\n",
       " ('an', 316),\n",
       " ('senator', 10),\n",
       " ('another.', 3),\n",
       " ('https://t.co/cbjQTK0Q1V', 1),\n",
       " ('Johnson', 27),\n",
       " ('vs.', 10),\n",
       " ('in', 3794),\n",
       " ('#Florida...', 1),\n",
       " ('#HillaryClinton', 21),\n",
       " ('https://t.co/zIeNEcVnMO', 1),\n",
       " ('bob', 1),\n",
       " ('is', 3478),\n",
       " ('person', 62),\n",
       " ('.', 179)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combines the counts by adding up the value (1) of equal keys (words)\n",
    "word_counts = counts.reduceByKey(lambda a, b: a + b)\n",
    "word_counts.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read the list of common [transformations](https://spark.apache.org/docs/1.6.2/programming-guide.html#transformations) and [actions](https://spark.apache.org/docs/1.6.2/programming-guide.html#actions) to get an idea of what you can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics\n",
    "\n",
    "Your turn! Try to extract some statisctis from the dataset:\n",
    "\n",
    "1. Total number of words\n",
    "2. Average number of words per tweet\n",
    "3. Ten most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300882"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tot number of words\n",
    "Tot=words.count()\n",
    "Tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294.11730205278593"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Average\n",
    "Ave=Tot/lines_with_trump.count()\n",
    "Ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Res=word_counts.sortBy(lambda a: a[1],False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Congress', 8526),\n",
       " ('the', 6494),\n",
       " ('to', 5829),\n",
       " ('of', 4654),\n",
       " ('for', 3964),\n",
       " ('in', 3794),\n",
       " ('congress', 3675),\n",
       " ('is', 3478),\n",
       " ('and', 3295),\n",
       " ('a', 2985)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Res.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "A bigram is combination of two consecutive words. \n",
    "\n",
    "- Extract the ten most frequent bigrams from the dataset. \n",
    "\n",
    "Be careful not to consider the last word of a tweet and the first one of the next tweet!\n",
    "\n",
    "*Hint:* note that the `map` and `flatMap` methods take a **function** as argument, meaning that you can define a function that you pass as argument:\n",
    "\n",
    "```python\n",
    "def process(line):\n",
    "    # Do something\n",
    "    return something\n",
    "\n",
    "x = tweets.flatMap(process)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(line):\n",
    "    words = line.split()\n",
    "    return [a + \" \" + b for a,b in zip(words, words[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ret = tweets.flatMap(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Lawrence @HillaryClinton',\n",
       " '@HillaryClinton Two',\n",
       " 'Two first',\n",
       " 'first @SenSchumer',\n",
       " '@SenSchumer tomorrow.',\n",
       " 'tomorrow. @TheLastWord',\n",
       " '@TheLastWord #brooklyn',\n",
       " '#brooklyn TheRealAmerica',\n",
       " 'TheRealAmerica #Vote',\n",
       " '#Vote #Democrats']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = ret.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@HillaryClinton Two', 1),\n",
       " ('Two first', 1),\n",
       " ('first @SenSchumer', 1),\n",
       " ('@SenSchumer tomorrow.', 1),\n",
       " ('#brooklyn TheRealAmerica', 1),\n",
       " ('#Democrats #NastyWomenVote', 1),\n",
       " ('#NastyWomenVote #Senate', 1),\n",
       " ('op-ed on', 1),\n",
       " ('historic #California', 1),\n",
       " ('#California #Senate', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq=Res=count.sortBy(lambda a: a[1],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of Congress', 677),\n",
       " ('of the', 542),\n",
       " ('Congress is', 518),\n",
       " ('in the', 429),\n",
       " ('in Congress', 410),\n",
       " ('for Congress', 366),\n",
       " ('vote for', 357),\n",
       " ('up for', 344),\n",
       " ('can do', 305),\n",
       " ('for grabs', 293)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.take(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
