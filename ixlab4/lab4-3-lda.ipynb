{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *F*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Dessimoz Frank*\n",
    "* *Micheli Vincent*\n",
    "* *Lefebvre Hippolyte*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import pickle\n",
    "from utils import load_json, load_pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = load_pkl('terms.pkl')\n",
    "words_prep=load_pkl('words_prep.pkl')\n",
    "courses= load_json('data/courses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ids_courses = list({c['courseId']:c for c in courses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[11569]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We load term frequency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tf.pkl', 'rb') as matrix:\n",
    "    tf = pickle.load(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=np.matrix(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect the columns of the term frequency matrix as arrays. So that we have the counts for all words in the corpus but for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=[]\n",
    "i=0\n",
    "for course in Ids_courses:\n",
    "    v = mat[:,i]\n",
    "    vect.append(v)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a collection of arrays corresponding to each document, we process the data so that they are in the good format for LDA method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsedData = rdd.map(lambda line: Vectors.dense([float(x) for x in line]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaModel = LDA.train(corpus, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordNumbers = 10  # number of words per topic\n",
    "topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print topic and words obtained via LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  1 :\n",
      "student\n",
      "report\n",
      "method\n",
      "problem\n",
      "model\n",
      "optim\n",
      "skill\n",
      "learn\n",
      "evalu\n",
      "risk\n",
      "\n",
      "Topic  2 :\n",
      "project\n",
      "student\n",
      "plan\n",
      "time\n",
      "learn\n",
      "assess\n",
      "evalu\n",
      "method\n",
      "system\n",
      "skill\n",
      "\n",
      "Topic  3 :\n",
      "method\n",
      "optic\n",
      "energi\n",
      "model\n",
      "equat\n",
      "basic\n",
      "physic\n",
      "numer\n",
      "analysi\n",
      "learn\n",
      "\n",
      "Topic  4 :\n",
      "design\n",
      "data\n",
      "learn\n",
      "model\n",
      "circuit\n",
      "digit\n",
      "student\n",
      "method\n",
      "system\n",
      "teach\n",
      "\n",
      "Topic  5 :\n",
      "system\n",
      "learn\n",
      "method\n",
      "comput\n",
      "student\n",
      "program\n",
      "process\n",
      "cours\n",
      "exercis\n",
      "control\n",
      "\n",
      "Topic  6 :\n",
      "student\n",
      "work\n",
      "studi\n",
      "learn\n",
      "case\n",
      "present\n",
      "develop\n",
      "method\n",
      "architectur\n",
      "content\n",
      "\n",
      "Topic  7 :\n",
      "model\n",
      "method\n",
      "learn\n",
      "network\n",
      "linear\n",
      "control\n",
      "analysi\n",
      "dynam\n",
      "content\n",
      "system\n",
      "\n",
      "Topic  8 :\n",
      "chemic\n",
      "chemistri\n",
      "biolog\n",
      "method\n",
      "reaction\n",
      "molecular\n",
      "protein\n",
      "cell\n",
      "student\n",
      "mass\n",
      "\n",
      "Topic  9 :\n",
      "materi\n",
      "electron\n",
      "mechan\n",
      "properti\n",
      "applic\n",
      "devic\n",
      "magnet\n",
      "physic\n",
      "method\n",
      "structur\n",
      "\n",
      "Topic  10 :\n",
      "student\n",
      "research\n",
      "week\n",
      "present\n",
      "field\n",
      "learn\n",
      "scientif\n",
      "discuss\n",
      "structur\n",
      "epfl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    arr=ldaModel.describeTopics(10)[i][0]\n",
    "    print('Topic ', i+1,':')\n",
    "    for x in arr:\n",
    "        print(terms[x])\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can label the topics as following:  \n",
    "\n",
    "    1) Risk modelling and visualization  \n",
    "    2) Problem solving skills  \n",
    "    3) Optic and Energy Physics  \n",
    "    4) Digital circuits design and learning  \n",
    "    5) MAN at work  \n",
    "    6) Architecture  \n",
    "    7) Networks Analysis  \n",
    "    8) Molecular and Cellular Biology  \n",
    "    9) Electrical engineering  \n",
    "    10)Students thesis presentations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each topic is still to generalist into its word distribution, the model suffer from under-parametrization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distributions in the LDA model have each one hyperparameter:\n",
    "    - α for the distribution of topics in documents \n",
    "    - β for the distribution of words in topics\n",
    "\n",
    "Their role is to encode any prior belief we have about the hidden topics for the corpus at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We Fix k = 10 and β = 1.01, and vary α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[0.01,0.15,0.5,1,1.2,50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we loop over those different values of alpha using the exact same method as before: (we will not output all the results here for reading convenience). This time weed parameters as following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaModel2 = LDA.train(corpus, k=10,topicConcentration=1.01,docConcentration=a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since b=1.01 words distribution over topics is close to uniform.   \n",
    "- When a=1, topics distribution is uniform. \n",
    "- When a>>1, topics distribution converges towards an equiprobable topics distribution. \n",
    "- When a<<1 topics distribution converges toward one dominant topic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We Fix k = 10 and α = 6, and vary β."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=[0.01,0.15,0.5,1,1.2,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaModel3 = LDA.train(corpus, k=10,docConcentration=6,topicConcentration=b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again the method is the same, thus we we will provide here only interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a=6 topics distribution is closer to an equiprobable topics distribution. \n",
    "- When b=1, words distribution is uniform. \n",
    "- When b>>1, words distribution converges towards an equiprobable words distribution. \n",
    "- When b<<1 words distribution converges toward one dominant word distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the combination of k, α and β that gives the most interpretable topics that most explain the topics covered by the courses taught at EPFL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We chose these values using the following assumptions: A broad number of subjects is discussed at EPFL which motivates an important number of topics. Engineering courses should be rather specific about one particular science related subject.  Hence, we should observe one or two dominant topic per course description. Therefore setting a < 1 to foster such distributions makes sense. Moreover, we should observe a large variety of topic related words in each topic. Therefore setting b > 1 to foster such distributions makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaModel4 = LDA.train(corpus, k=30,docConcentration=0.15,topicConcentration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of topics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    arr=ldaModel.describeTopics(10)[i][0]\n",
    "    print('Topic ', i+1,':')\n",
    "    for x in arr:\n",
    "        print(terms[x])\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of parameters :\n",
    "- k = 30\n",
    "- α = 0.15\n",
    "- β = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since the Wikipedia for school is large and a large collection of subjects is covered, the topic number k should be quite large. As we said previously, each document should concentrate on a few topics. Therefore setting a < 1 to foster dominant topics distributions makes sense.  Moreover, we should observe a large variety of topic related words in each topic. Therefore setting b>1 to foster close to equiprobable words distributions makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of parameters chosen a priori:\n",
    "- k = 100\n",
    "- α = 0.01\n",
    "- β = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
