{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *F*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Dessimoz Frank*\n",
    "* *Micheli Vincent*\n",
    "* *Lefebvre Hippolyte*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import pickle\n",
    "from utils import load_json, load_pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = load_pkl('terms.pkl')\n",
    "words_prep=load_pkl('words_prep.pkl')\n",
    "courses= load_json('data/courses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ids_courses = list({c['courseId']:c for c in courses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We load term frequency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tf.pkl', 'rb') as matrix:\n",
    "    tf = pickle.load(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=np.matrix(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect the columns of the term frequency matrix as arrays. So that we have the counts for all words in the corpus but for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=[]\n",
    "i=0\n",
    "for course in Ids_courses:\n",
    "    v = mat[:,i]\n",
    "    vect.append(v)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a collection of arrays corresponding to each document, we process the data so that they are in the good format for LDA method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsedData = rdd.map(lambda line: Vectors.dense([float(x) for x in line]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaModel = LDA.train(corpus, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordNumbers = 10  # number of words per topic\n",
    "topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 11819 words):\n",
      "Topic 0:\n",
      " 0.0308961754174\n",
      " 0.0188959436342\n",
      " 21.7786494634\n",
      " 0.0223688164579\n",
      " 137.972770856\n",
      " 0.467983226446\n",
      " 0.00320211011815\n",
      " 0.0394497374966\n",
      " 0.0246049873522\n",
      " 5.81887873418\n",
      "Topic 1:\n",
      " 0.0328086922443\n",
      " 0.0196806902237\n",
      " 0.946783511093\n",
      " 0.727501463036\n",
      " 63.0680442743\n",
      " 4.35235894697\n",
      " 0.00867648652684\n",
      " 0.035969705229\n",
      " 0.00659869555561\n",
      " 11.2491353688\n",
      "Topic 2:\n",
      " 0.418444976835\n",
      " 0.0246850881309\n",
      " 17.3031151231\n",
      " 0.0304643272094\n",
      " 141.556224903\n",
      " 0.104675284554\n",
      " 0.00235571182403\n",
      " 1.75115495308\n",
      " 0.0289719803113\n",
      " 9.41123887162\n",
      "Topic 3:\n",
      " 0.0343855635374\n",
      " 0.0197289215763\n",
      " 1.7019673608\n",
      " 0.0388422643016\n",
      " 63.1923154455\n",
      " 1.29082309444\n",
      " 0.0252250219016\n",
      " 0.0360986521766\n",
      " 0.00853787897972\n",
      " 48.9773778965\n",
      "Topic 4:\n",
      " 0.0540493392075\n",
      " 0.0235345472289\n",
      " 4.52448248573\n",
      " 0.0335918266551\n",
      " 55.2483221461\n",
      " 1.65340913713\n",
      " 0.00351339948945\n",
      " 0.110177017849\n",
      " 0.0155464775089\n",
      " 5.23084168431\n",
      "Topic 5:\n",
      " 0.0734286708979\n",
      " 0.0306413425516\n",
      " 4.69292901428\n",
      " 0.0232280226144\n",
      " 17.9096744227\n",
      " 1.4028836451\n",
      " 1.94463429292\n",
      " 0.0306582766247\n",
      " 0.0117741673294\n",
      " 10.755556542\n",
      "Topic 6:\n",
      " 0.098762819593\n",
      " 0.0293761604836\n",
      " 15.0612457873\n",
      " 0.0797617495813\n",
      " 130.934734386\n",
      " 0.419787559577\n",
      " 0.00346404089926\n",
      " 0.0415344315752\n",
      " 0.735877921685\n",
      " 1.65818000845\n",
      "Topic 7:\n",
      " 0.0736253729251\n",
      " 0.033980876391\n",
      " 1.19327028887\n",
      " 0.0117493001434\n",
      " 34.6253148229\n",
      " 6.24690939888\n",
      " 0.00250870848725\n",
      " 0.840923946057\n",
      " 0.0843397679827\n",
      " 6.04138352759\n",
      "Topic 8:\n",
      " 0.0452636017225\n",
      " 0.765384777667\n",
      " 3.34844509218\n",
      " 0.0106641904872\n",
      " 12.7095853948\n",
      " 2.70261790811\n",
      " 0.00275519039278\n",
      " 0.0908266092736\n",
      " 0.072973279975\n",
      " 26.0055511446\n",
      "Topic 9:\n",
      " 0.13833478762\n",
      " 0.0340916521133\n",
      " 0.449111873266\n",
      " 0.0218280395141\n",
      " 61.7830133489\n",
      " 19.3585517988\n",
      " 0.00366503744088\n",
      " 0.0232066706411\n",
      " 0.0107748433198\n",
      " 6.85185622195\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize()) + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(10):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, 10):\n",
    "        print(\" \" + str(topics[word][topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
