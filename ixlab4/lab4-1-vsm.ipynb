{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *F*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Dessimoz Frank*\n",
    "* *Micheli Vincent*\n",
    "* *Lefebvre Hippolyte*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import collections \n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's gather all the courses in a list. We pay attention to not include several times the same ID, we thus look for unique ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ids_courses = list({c['courseId']:c for c in courses})#list of course ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to that list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSE-440', 'BIO-695', 'FIN-523', 'MICRO-614', 'ME-231(a)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ids_courses[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ids_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a course called \"Caution, these contents corresponds to the coursebooks of last year\". Let's create two list of courses, one with and one without this \"weird\" ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "courses_with =list({c['courseId']:c for c in courses}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "courses_without = list({c['courseId']:c for c in courses if not c['courseId']== 'Caution, these contents corresponds to the coursebooks of last year'}.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same way we create a list of unique stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords)#get unique word from stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at stop dictionnary, we notice that there is no ponctuation. Let's create our own dictionnary of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punct=[]\n",
    "for c in string.punctuation:\n",
    "    punct.append(c)#create the array of punctuation character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '#', '$', '%']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct[0:5]#6 first characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our stop words list with punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop.update(punct)#update stop words list with punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to remove stop words and punctuation and compute word frequency for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize #ntlk library to tokenize each course description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#We create a dict with course Id as key and words as value\n",
    "word_dict = {}\n",
    "\n",
    "for id, course in enumerate(courses_without):\n",
    "    \n",
    "    desc = course['description'] #get the dexcription of the current course\n",
    "    w = []\n",
    "    for words in wordpunct_tokenize(desc):# tokenize (split) words of the description\n",
    "        \n",
    "        word = words.lower() #lower the words to avoid mistakes\n",
    "        \n",
    "        if word not in stop:#if word not in stopwords\n",
    "             \n",
    "            w.append(word)#We store this word in a list    \n",
    "         \n",
    "    #We notice some wrong strings containing two punctuation, we need to split and remove         \n",
    "    w = [''.join(c for c in s if c not in punct) for s in w]#remove some words containing several punctuations\n",
    "    w = [s for s in w if s]#remove empty strings caused by the line above\n",
    "   \n",
    "    word_dict[course['courseId']]=w#for each courses, we provide the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "853"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MGT-641(b)', 'FIN-504', 'COM-303', 'MSE-231', 'COM-308']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(word_dict.keys()))[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the list of words of the first course called \"AR-201(c)\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house',\n",
       " 'simple',\n",
       " 'topic',\n",
       " 'studio',\n",
       " 'matter',\n",
       " 'simple',\n",
       " 'complexity',\n",
       " 'defining',\n",
       " 'space']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['AR-201(c)'][0:9]#9 first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still notice some wrong words such as simple numbers. We decide to overlook them, the number of them is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform stemming to remove suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *# We use porter algo : http://snowball.tartarus.org/algorithms/porter/stemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()#stemming function\n",
    "stemmed_words = []#array to collect stemmed words\n",
    "word_dict_stem={}#dict to store stemmer words for each course\n",
    "\n",
    "for course, val in word_dict.items():#we loop over each course in the dictionnary of words\n",
    "    words = word_dict[course]#we get the current course word's list\n",
    "    \n",
    "    temp = []\n",
    "    for w in words:#loop over words\n",
    "        temp.append(stemmer.stem(w))#we store the stemmed part of the word\n",
    "        \n",
    "    stemmed_words+=temp#we fill the list for all stemmed words that will be used later for the matrix\n",
    "    word_dict_stem[course] = temp#we fill the dict of stemmed words for each course\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we want to compute the count of each word in each course description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict_count={}#create dictionnary of counts\n",
    "for course, val in word_dict_stem.items():#loop over each course\n",
    "    count=collections.Counter(val)#Counter methods returns an array with the count of each word\n",
    "    word_dict_count[course]=dict(count)#fill the dict of words count per course\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And eventually the frequency, which is simply those counts divided by the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict_freq={}#Not really relevant, TF \"frequency\" is actually counts\n",
    "for course, val in word_dict_count.items():\n",
    "   \n",
    "    total = sum(val.values(), 0.0)\n",
    "    new_val = {k: v / total for k, v in val.items()}\n",
    "    word_dict_freq[course]=new_val\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the TF matrix that will gather the frequency (which is the counting) of each words in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms=list(set(stemmed_words))#create a list of unique stemmed words, all stemmed words\n",
    "classes = list(set(word_dict.keys()))#create a list of unique courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dump the list of words for later usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"terms.pkl\", \"wb\") as list_terms:\n",
    "    pickle.dump(terms, list_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Matrix_TF = np.zeros((len(terms), len(classes)))#create matrix of term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size of marix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11816, 853)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix_TF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fill the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idc, course in enumerate(word_dict_count):#loop over all courses\n",
    "    \n",
    "    for word, freq in word_dict_count[course].items():#loop over each key:value\n",
    "        \n",
    "        idw=terms.index(word)#check position of current word in terms array defined above to position in matrix\n",
    "        Matrix_TF[idw][idc] = freq#fill the matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first word to be processed is \"latest\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11481"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms.index('latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix_TF[11481][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the inverse document frequency: first we compute counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict_inv_count={key:0 for key in terms}# we create a dict with our list of stemmed words as keys and null values\n",
    "for word in terms:#for each words in the stemmed list\n",
    "   \n",
    "    for k,v in word_dict_freq.items():#we loop over all doc to find if they contain at least once the word\n",
    "       \n",
    "        if word in v:#the key exists in that doc\n",
    "            word_dict_inv_count[word]+=1#We add one each time a topic appears at least once in a doc\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply log to the inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict_inv_freq={}\n",
    "for k, v in word_dict_inv_count.items():\n",
    "    word_dict_inv_freq[k]=np.log(len(classes)/v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11816"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict_inv_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the IDF vector  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDF=list(word_dict_inv_freq.values())#IDF vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute the TFIDF matrix which is simply the product of TF matrix and IDF vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TF_IDF = Matrix_TF * np.array([IDF]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"tf_idf.pkl\", \"wb\") as tfidf:\n",
    "    pickle.dump(TF_IDF, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look to the results for our Internet Analytics class: COM 308:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_IA = Ids_courses.index('COM-308')#find the index of this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to retrieve all the words for this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_IA = {}#store words as key and scores as values\n",
    "for idx, value in enumerate(TF_IDF[:,index_IA]):\n",
    "    \n",
    "    scores_IA[terms[idx]] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Scores_ordered_IA = OrderedDict(sorted(scores_IA.items(),key = operator.itemgetter(1),reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mine', 18.677272023247372),\n",
       " ('onlin', 17.453315047350987),\n",
       " ('social', 15.689208174237272),\n",
       " ('explor', 15.055449646041554),\n",
       " ('world', 14.38779268291894),\n",
       " ('hadoop', 12.111224733863468),\n",
       " ('real', 11.414257144185477),\n",
       " ('servic', 10.839795994687588),\n",
       " ('auction', 10.724930372743577),\n",
       " ('commerc', 10.724930372743577),\n",
       " ('retriev', 9.6056987968727316),\n",
       " ('internet', 9.6056987968727316),\n",
       " ('network', 9.3681612009097037),\n",
       " ('dataset', 8.527705795407357)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Scores_ordered_IA.items())[0:14]#15 first topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we are interested in the two follwing subjects : Markov chain and Facebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markov: True\n",
      "chain: True\n",
      "facebook: True\n"
     ]
    }
   ],
   "source": [
    "for word in ['markov','chain','facebook']:\n",
    "    if word in terms:\n",
    "        print(word + ': True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thus the 3 words belong to our word list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we need to create a vector of Markov chain and Facebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MC = np.zeros(len(terms))\n",
    "FB= np.zeros(len(terms))\n",
    "MC_FB=np.zeros(len(terms))\n",
    "\n",
    "#For \"markov chain\" : two words\n",
    "MC[terms.index('markov')] = 1/2\n",
    "MC[terms.index('chain')] = 1/2\n",
    "\n",
    "#For \"facebook\"\n",
    "FB[terms.index('facebook')] = 1\n",
    "\n",
    "#For markov chain and facebook = 3 words\n",
    "MC_FB[terms.index('markov')] = 1/3\n",
    "MC_FB[terms.index('chain')] = 1/3\n",
    "MC_FB[terms.index('facebook')] = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now for this 3 situation we want to find the topics most relevant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a cosine similarity function (we could also use sklearn method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(di, dj):#compute closeness between a topic and a document\n",
    "    score= np.dot(di, dj) / (np.linalg.norm(di) * np.linalg.norm(dj))# Frobenius norm byt default\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a function to get the 5 top matches by applying the cosine function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_five(vect):\n",
    "   \n",
    "    cosine_score=np.apply_along_axis(cosine_similarity, 0, TF_IDF, vect)#apply function columnwise (each column is a doc)\n",
    "    top5 = np.argsort(cosine_score)[-5:][::-1]#sort and get five best score\n",
    "    print(top5)\n",
    "    temp = []\n",
    "    for top in top5:\n",
    "        temp.append((courses_without[top]['courseId'],courses_without[top]['name'], cosine_score[top]))\n",
    "        #we retrieve the courses in the original course dataframe\n",
    "    \n",
    "    print(\"Five top matches :\")\n",
    "    iterator=1\n",
    "    for idx,name,score in temp:\n",
    "        print(str(iterator)+') ', idx,':',name, ':', score) \n",
    "        iterator+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Markov chain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 80 398  44 245  99]\n",
      "Five top matches :\n",
      "1)  MATH-332 : Applied stochastic processes : 0.552014373903\n",
      "2)  MGT-484 : Applied probability & stochastic processes : 0.545140925407\n",
      "3)  MGT-526 : Supply chain management : 0.37517637849\n",
      "4)  COM-516 : Markov chains and algorithmic applications : 0.375059076082\n",
      "5)  MGT-602 : Mathematical models in supply chain management : 0.31014411071\n"
     ]
    }
   ],
   "source": [
    "top_five(MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For facebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five top matches :\n",
      "1)  EE-727 : Computational Social Media : 0.178554615848\n",
      "2)  ENV-523 : Hydrogeophysics : 0.0\n",
      "3)  PHYS-615 : Electronic properties of solids and superconductivity : 0.0\n",
      "4)  MSE-656 : CCMX Advanced Course - Instrumented Nanoindentation : 0.0\n",
      "5)  CH-312 : Molecular and cellular biophysic II : 0.0\n"
     ]
    }
   ],
   "source": [
    "top_five(FB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five top matches :\n",
      "1)  MATH-332 : Applied stochastic processes : 0.450717848915\n",
      "2)  MGT-484 : Applied probability & stochastic processes : 0.445105701718\n",
      "3)  MGT-526 : Supply chain management : 0.306330230282\n",
      "4)  COM-516 : Markov chains and algorithmic applications : 0.306234453267\n",
      "5)  MGT-602 : Mathematical models in supply chain management : 0.253231605989\n"
     ]
    }
   ],
   "source": [
    "top_five(MC_FB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We notice that for markov chain, finding topics is rather easy while facebook matching only retrieve one match. When we try to get a match for markov chain and facebook at the same time, we get the same results than for only markov chain. At EPFL, since there are a lot of classes dealing with markov chain,these two words appear a lot in course description. Whereas facebook doesn't appear a lot, but social media does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Soc_media=np.zeros(len(terms))\n",
    "Soc_media[terms.index('social')] = 1/2\n",
    "Soc_media[terms.index('media')] = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five top matches :\n",
      "1)  EE-727 : Computational Social Media : 0.759554271309\n",
      "2)  EE-593 : Social media : 0.402063305318\n",
      "3)  HUM-432(a) : How people learn I : 0.24609192435\n",
      "4)  COM-308 : Internet analytics : 0.215365138079\n",
      "5)  EE-552 : Media security : 0.186856249464\n"
     ]
    }
   ],
   "source": [
    "top_five(Soc_media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can say that vector space retrieval models use a lot word direct matching instead of understanding theme or context. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
